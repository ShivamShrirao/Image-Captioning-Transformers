{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae2eb10-f8d5-4f47-bccf-c29faf2cdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb4177c-640f-469c-9ac4-d09a19ee8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6332628b-660d-4867-b690-e0642674c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98a58c51-77fb-48f7-a6bf-52e6dcd8f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try pre trained CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39cda7a8-5913-4ff8-a871-d56f9cc16750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.utils import download_and_extract_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef7b85f2-5b20-48d0-a94a-8bfc4221309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_extract_archive(\"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "#                              download_root=\"../datasets/COCO\",\n",
    "#                              remove_finished=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e4007d9-9bc7-4d75-82ee-295e43b312cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_extract_archive(\"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "#                              download_root=\"../datasets/COCO\",\n",
    "#                              remove_finished=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e91386a8-b174-4720-af37-a6b2704eb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_extract_archive(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "#                              download_root=\"../datasets/COCO\",\n",
    "#                              remove_finished=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da9a4f51-0e2b-40fc-ac16-258f576b2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a236395a-fe32-4980-a822-ef25a85ce00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1bfd1dc-996b-4b19-88f0-0653ff24989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = {\n",
    "    'train': T.Compose([\n",
    "        T.RandomResizedCrop(input_size, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.RandomHorizontalFlip(input_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': T.Compose([\n",
    "        T.Resize(input_size, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(input_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c58a2731-9751-47bf-a2f1-55a9c008ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, d_model=768, dp_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.LazyLinear(d_model)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dp_rate)\n",
    "        self.dense = nn.LazyLinear(d_model)\n",
    "        self.dropout2 = nn.Dropout(dp_rate)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):   # (..., features)\n",
    "        p = self.proj(x)\n",
    "        x = self.dropout1(self.activation(p))\n",
    "        x = self.dropout2(self.dense(x))\n",
    "        x = self.ln(x + p)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bd77da0-6fa1-4898-93d2-284e362a2f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn((1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56a4b971-a4f9-463b-92ab-17ab738e941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model('seresnext50_32x4d', pretrained=False, num_classes=0, global_pool='')#, features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdb21928-2cb6-4767-9ce0-7db5746be3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = backbone(inp).flatten(-2).permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "356b3d94-866c-4528-bfcb-7d1ec7b95e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 1, 2048])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3cd11bf-e7c2-4e98-b8b1-aa37fec2e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = ProjectionHead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b9131a7-aaa4-4294-a456-abf6036b77bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 1, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791438db-b1d3-40ac-ae72-9dc0068b81a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "123309be-3f25-4428-9800-9767b63daaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fastai/fastai2/blob/8d798c881c1eda564bdf92079bdfe43b43525767/fastai2/callback/training.py\n",
    "bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "\n",
    "def set_bn_eval(m:nn.Module):\n",
    "    \"Set bn layers in eval mode for all recursive children of `m`.\"\n",
    "    for l in m.children():\n",
    "        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:\n",
    "            l.eval()\n",
    "        set_bn_eval(l)\n",
    "\n",
    "def freeze_weights(m):\n",
    "    for param in m.parameters():\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69003d27-7265-4a01-9aeb-07143fba2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, projection_head, tgt_vocab_size, num_decoder_layers=6, nhead=8, d_model=768,\n",
    "                 dim_feedforward=2048, dp_rate=0.1, activation='relu', bn_eval=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('seresnext50_32x4d', pretrained=False, num_classes=0, global_pool='')\n",
    "        freeze_weights(self.backbone)\n",
    "        if bn_eval: set_bn_eval(self.backbone)\n",
    "        \n",
    "        self.projection_head = ProjectionHead(d_model, dp_rate)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dp_rate,\n",
    "                                                activation)\n",
    "        decoder_norm = LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        self.generator = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.scale = d_model**0.5\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, x, tgt):   # x[B,C,H,W]  tgt[B,seq_len]\n",
    "        x = self.backbone(x)\n",
    "        # (B, features, h, w)\n",
    "        x = x.flatten(-2)    # flatten each feature\n",
    "        # (B, features, h*w) cause batch_first.\n",
    "        x = x.permute(2,0,1)\n",
    "        # (h*w, B, features)\n",
    "        x = self.projection_head(x)\n",
    "        # (h*w, B, d_model)\n",
    "\n",
    "        tgt = self.embedding(tgt)*self.scale\n",
    "        # (seq_len, B, d_model)\n",
    "        tgt = tgt + self.pos_enc[:tgt.size(0), :]\n",
    "        x = self.decoder(tgt, memory=x, tgt_mask=tgt_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        x = self.generator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1537aa7-4ebb-4c63-9dc8-ac60fe1d4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(sz):\n",
    "    mask = torch.ones((1,1,sz,sz), device=device, dtype=bool).triu(1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7734054-af8a-4823-8b50-66f2e6c86b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=CONFIG['lr'], betas=CONFIG['betas'], eps=CONFIG['eps']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5914e791-b1fd-405b-aa65-bdd1d9e13e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4e10704-bd29-465c-8626-b0a48ee2f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96ca3492-26a4-4310-bec7-44cec5092a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cap_data = datasets.CocoCaptions(root=\"../datasets/COCO/val2017/\",\n",
    "                                 annFile=\"../datasets/COCO/annotations/captions_val2017.json\",\n",
    "                                 transform=preproc['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c84dd7aa-6686-4e2f-8537-919460d0a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(cap_data):\n",
    "    for ann in cap_data.coco.anns.values():\n",
    "        yield tokenizer(ann['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f9160b18-b104-4635-b001-a8f871ebe754",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f8922a7a-5747-4a04-95fc-976a6947a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = build_vocab_from_iterator(yield_tokens(cap_data), specials=special_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5bc8bcae-20c3-46fe-b87b-8da97ba7eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = en_vocab(special_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bdfb10a7-fed3-4f3e-a0c1-bd7ecdcae303",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15bf46e8-6cd7-4555-94dd-8764757d79dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 44, 11, 24, 45, 50, 15, 23, 13, 4, 113, 11, 24, 45, 50, 5]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab(tokenizer('A black and white small dog sitting next to a brown and white small dog.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa971ef2-aba8-49da-a8ef-97e898697517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer('A black and white small dog sitting next to a brown and white small dog.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "893f50d5-adff-459f-a09c-10e001e48d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec = torchtext.vocab.GloVe('6B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "042950b8-a6d3-494d-997f-1edbc2e53339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2971,  0.0940, -0.0967,  ...,  0.0597, -0.2285,  0.2960],\n",
       "        [-0.0868,  0.0362,  0.4657,  ...,  0.0679,  0.0267,  0.2247],\n",
       "        [ 0.0385, -0.0398,  0.0827,  ..., -0.3343,  0.0118,  0.0597],\n",
       "        ...,\n",
       "        [-0.4330,  0.3283, -0.0943,  ..., -0.1941, -0.1111, -0.0581],\n",
       "        [-0.1104,  0.8122,  0.0737,  ...,  0.3394,  0.5799,  0.0681],\n",
       "        [-0.1256,  0.0136,  0.1031,  ..., -0.3422, -0.0224,  0.1368]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_vecs_by_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5214ef6-ef1f-4ae6-8b26-d007691a1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "#     en_batch = pad_sequence(en_batch, batch_first=False, padding_value=PAD_IDX)\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88b4a339-32b8-4479-a0c3-f062ba305458",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cap_data,\n",
    "                                         batch_size=2,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=3,\n",
    "                                         collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1160dac-52e0-4c78-bac6-b2641f0ecbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a15794ed-6c8f-4aa9-bb28-b9437491a669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two small dogs on leashes walking on a brick pathway.',\n",
       " 'Two different colored dogs standing on a brick walkway.',\n",
       " 'a black and white dog and a brown and white dog both on leashes',\n",
       " 'A black and white small dog sitting next to a brown and white small dog.',\n",
       " 'TWO PET DOGS ARE ON A LEASH ']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d397e4d-caa9-402a-8831-63cb79723e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7840ce0e-dee1-4323-9eeb-05fda2898d8f",
   "metadata": {},
   "source": [
    "# Image Captioning with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae2eb10-f8d5-4f47-bccf-c29faf2cdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6332628b-660d-4867-b690-e0642674c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import randint\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc18b9ba-75cb-4b7a-a909-cb226522e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb4177c-640f-469c-9ac4-d09a19ee8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a58c51-77fb-48f7-a6bf-52e6dcd8f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try pre trained CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819264fd-bf0c-4862-9dbd-c4d2504a7071",
   "metadata": {},
   "source": [
    "# Download Dataset and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39cda7a8-5913-4ff8-a871-d56f9cc16750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.utils import download_and_extract_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7b85f2-5b20-48d0-a94a-8bfc4221309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_extract_archive(\"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "#                              download_root=\"../datasets/COCO\",\n",
    "#                              remove_finished=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4007d9-9bc7-4d75-82ee-295e43b312cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_extract_archive(\"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "#                              download_root=\"../datasets/COCO\",\n",
    "#                              remove_finished=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e91386a8-b174-4720-af37-a6b2704eb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_extract_archive(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "#                              download_root=\"../datasets/COCO\",\n",
    "#                              remove_finished=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da9a4f51-0e2b-40fc-ac16-258f576b2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45cb594-a2d0-477c-8df2-007a76c106dc",
   "metadata": {},
   "source": [
    "# Preprocessing Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a236395a-fe32-4980-a822-ef25a85ce00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1bfd1dc-996b-4b19-88f0-0653ff24989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = {\n",
    "    'train': T.Compose([\n",
    "        T.RandomResizedCrop(input_size, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.RandomHorizontalFlip(input_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': T.Compose([\n",
    "        T.Resize(input_size, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(input_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f8a1e-3a8c-40d1-b631-7acfe236e622",
   "metadata": {},
   "source": [
    "# Define Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c58a2731-9751-47bf-a2f1-55a9c008ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, d_model=512, dp_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.LazyLinear(d_model)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dp_rate)\n",
    "        self.dense = nn.LazyLinear(d_model)\n",
    "        self.dropout2 = nn.Dropout(dp_rate)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):   # (..., features)\n",
    "        p = self.proj(x)\n",
    "        x = self.dropout1(self.activation(p))\n",
    "        x = self.dropout2(self.dense(x))\n",
    "        x = self.ln(x + p)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bd77da0-6fa1-4898-93d2-284e362a2f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.randn((1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56a4b971-a4f9-463b-92ab-17ab738e941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = timm.create_model('seresnext50_32x4d', pretrained=False, num_classes=0, global_pool='')#, features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdb21928-2cb6-4767-9ce0-7db5746be3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = encoder(inp).flatten(-2).permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "356b3d94-866c-4528-bfcb-7d1ec7b95e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3cd11bf-e7c2-4e98-b8b1-aa37fec2e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ph = ProjectionHead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b9131a7-aaa4-4294-a456-abf6036b77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ph(out).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c8f90-0cba-4313-b74f-06b65542c906",
   "metadata": {},
   "source": [
    "# Token Embedding and Positional Encoding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "791438db-b1d3-40ac-ae72-9dc0068b81a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.scale = emb_size**0.5\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * self.scale\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size, dp_rate, maxlen = 1_000):\n",
    "        super().__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10_000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9079c3-518c-4a97-ab32-ffebbf3ac3cc",
   "metadata": {},
   "source": [
    "# Freezing encoder parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "123309be-3f25-4428-9800-9767b63daaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fastai/fastai2/blob/8d798c881c1eda564bdf92079bdfe43b43525767/fastai2/callback/training.py\n",
    "bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "\n",
    "def set_bn_eval(m:nn.Module):\n",
    "    \"Set bn layers in eval mode for all recursive children of `m`.\"\n",
    "    for l in m.children():\n",
    "        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:\n",
    "            l.eval()\n",
    "        set_bn_eval(l)\n",
    "\n",
    "def freeze_weights(m):\n",
    "    for param in m.parameters():\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f79df-0490-4aa9-9845-319260664d88",
   "metadata": {},
   "source": [
    "# Model class with encoder, projection and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69003d27-7265-4a01-9aeb-07143fba2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_decoder_layers=6, nheads=8, d_model=512,\n",
    "                 dim_feedforward=2048, dp_rate=0.1, activation='relu', bn_eval=True):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model('seresnext50_32x4d', pretrained=False, num_classes=0, global_pool='')\n",
    "        freeze_weights(self.encoder)\n",
    "        if bn_eval: set_bn_eval(self.encoder)\n",
    "        \n",
    "        self.projection_head = ProjectionHead(d_model, dp_rate)\n",
    "\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, dp_rate)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nheads, dim_feedforward, dp_rate,\n",
    "                                                   activation)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "        self.generator = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, tgt, tgt_mask=None, tgt_key_padding_mask=None):   # x[B,C,H,W]  tgt[seq_len,B]\n",
    "        # Extract Image Features\n",
    "        x = self.encoder(x)\n",
    "        # (B, features, h, w)\n",
    "        x = x.flatten(-2)    # flatten each feature\n",
    "        # (B, features, h*w)\n",
    "        x = x.permute(2,0,1)\n",
    "        # (h*w, B, features)\n",
    "        x = self.projection_head(x)\n",
    "        # (h*w, B, d_model)\n",
    "\n",
    "        # Generate Captions\n",
    "        tgt = self.pos_enc(self.tok_emb(tgt))\n",
    "        # (seq_len, B, d_model)\n",
    "        x = self.decoder(tgt, memory=x, tgt_mask=tgt_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        # (seq_len, B, d_model)\n",
    "        x = self.generator(x)\n",
    "        # (seq_len, B, vocab_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f84c037-bb46-4abf-b2c9-92e130cccc35",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1537aa7-4ebb-4c63-9dc8-ac60fe1d4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(sz):\n",
    "    mask = torch.ones((sz,sz), device=device, dtype=bool)\n",
    "    mask.triu_(1)\n",
    "    return mask\n",
    "\n",
    "def padding_mask(tgt, pad_idx):\n",
    "    return (tgt==pad_idx).transpose(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe13be-2bbb-45f2-9a8a-5cd3a209d4e6",
   "metadata": {},
   "source": [
    "# Dataset utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5914e791-b1fd-405b-aa65-bdd1d9e13e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f199d-ba6f-4d12-a3b0-76a5f987f0c9",
   "metadata": {},
   "source": [
    "## Read COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96ca3492-26a4-4310-bec7-44cec5092a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.12s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cap_data = datasets.CocoCaptions(root=\"../datasets/COCO/val2017/\",\n",
    "                                 annFile=\"../datasets/COCO/annotations/captions_val2017.json\",\n",
    "                                 transform=preproc['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d1c495-f3d0-4b58-bb88-286530849969",
   "metadata": {},
   "source": [
    "## Tokenizer and Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4e10704-bd29-465c-8626-b0a48ee2f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c84dd7aa-6686-4e2f-8537-919460d0a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(cap_data):\n",
    "    for ann in cap_data.coco.anns.values():\n",
    "        yield tokenizer(ann['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8922a7a-5747-4a04-95fc-976a6947a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "en_vocab = build_vocab_from_iterator(yield_tokens(cap_data), specials=special_symbols, special_first=True)\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = en_vocab(special_symbols)\n",
    "en_vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d493c53-ce30-443e-936e-a1dac5270477",
   "metadata": {},
   "source": [
    "## Pretrained Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "893f50d5-adff-459f-a09c-10e001e48d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec = torchtext.vocab.GloVe('6B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "475fa54b-d223-4c12-b55b-92d83b797d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_vec = vec.vectors.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba5273de-16eb-472f-9126-fb8c15761124",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.unk_init = lambda x: unk_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "042950b8-a6d3-494d-997f-1edbc2e53339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec.get_vecs_by_tokens(tokens, lower_case_backup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7920ad-38a9-4fc7-9878-a0765ddac8fd",
   "metadata": {},
   "source": [
    "# Load dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5214ef6-ef1f-4ae6-8b26-d007691a1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    img_batch, cap_batch = [], []\n",
    "    for img, caps in data_batch:\n",
    "        img_batch.append(img)\n",
    "        cap = caps[randint(0,len(caps)-1)]\n",
    "        cap_batch.append(torch.tensor([BOS_IDX] + en_vocab(tokenizer(cap)) + [EOS_IDX]))\n",
    "\n",
    "    cap_batch = pad_sequence(cap_batch, batch_first=False, padding_value=PAD_IDX)\n",
    "    return torch.stack(img_batch), cap_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16a61e3a-1331-4bb2-87f0-7a302345b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88b4a339-32b8-4479-a0c3-f062ba305458",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cap_data,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=3,\n",
    "                                         pin_memory=True,\n",
    "                                         collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f978fc3-2bca-40cf-af8a-263371f97577",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6f8f439-401b-4dd3-ada8-fbbfcf2beded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b59efe92-566d-46ab-a91a-6323a855ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_defaults = {\n",
    "    'tgt_vocab_size'    : len(en_vocab),\n",
    "    'BATCH_SIZE'        : BATCH_SIZE,\n",
    "    'd_model'           : 512,\n",
    "    'dim_feedforward'   : 2048,\n",
    "    'nheads'            : 8,\n",
    "    'num_decoder_layers': 6,\n",
    "    'dp_rate'           : 0.1,\n",
    "    'activation'        : 'gelu',\n",
    "    'ilr'               : 1,\n",
    "    'betas'             : (0.9, 0.98),\n",
    "    'eps'               : 1e-9,\n",
    "    'use_amp'           : True,\n",
    "    'use_pe'            : True,\n",
    "    'log_interval'      : 5,\n",
    "}\n",
    "CONFIG = config_defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c04da7-ebc6-4ea8-b803-e5529df1668b",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c54e00a1-b9e4-4841-befe-6b55ec0f75fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = CaptionModel(vocab_size = CONFIG['tgt_vocab_size'],\n",
    "                     num_decoder_layers = CONFIG['num_decoder_layers'],\n",
    "                     nheads = CONFIG['nheads'],\n",
    "                     d_model = CONFIG['d_model'],\n",
    "                     dim_feedforward = CONFIG['dim_feedforward'],\n",
    "                     dp_rate = CONFIG['dp_rate'],\n",
    "                     activation = CONFIG['activation']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9faa68ed-ccb7-4ead-9e3f-672ed570d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cap = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c337fbde-660b-4f56-b131-ef9913a0c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.to(device)\n",
    "cap = cap.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cad2adf6-29c5-4db2-86ea-3f08d422bc47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tgt_mask, tgt_pad_mask = subsequent_mask(cap.size(0)), padding_mask(cap, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce5ecdc5-4940-4a6b-be0f-3fe890f8d4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25, 25]), torch.Size([64, 25]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_mask.shape, tgt_pad_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0275fc7e-0f6c-427d-9b8c-a9ed6838fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "o = model(img, cap, tgt_mask, tgt_pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce8b30f4-bd92-450f-9dab-a141189dd07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 64, 7316])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4b6bf-c036-429f-b5e5-1d053f3d91e9",
   "metadata": {},
   "source": [
    "# Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6336bdd7-aee5-4bd7-b382-cb18e023aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(step, d_model=1024, warmup_steps=4000):\n",
    "    # return 1\n",
    "    step = max(1,step)\n",
    "    arg1 = step ** -0.5\n",
    "    arg2 = step * (warmup_steps ** -1.5)\n",
    "    return (d_model ** -0.5) * min(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ceca2478-81ff-402f-b984-72c19e0391d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(map(lr_schedule, range(114*400))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37d126-9abb-4a3e-8304-cd589dee058e",
   "metadata": {},
   "source": [
    "# Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "530023ee-432b-4ed8-ba6d-1831dffaebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=CONFIG['ilr'], betas=CONFIG['betas'], eps=CONFIG['eps']\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_schedule)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=CONFIG['use_amp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f4ddf-43e1-4f85-bc51-99f2651c49ee",
   "metadata": {},
   "source": [
    "# Log model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a37e73c-d84a-4086-a7e0-6358c6b87f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3bfdae22-c7a6-49f9-a9e7-6bd1f3e35e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = SummaryWriter(run.dir+'/tensorboard_logs/imcap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8cbd25a9-e8f5-4b2f-bd7a-3f594ad2f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, tgt = next(iter(train_iter))\n",
    "# src = src.to(device)\n",
    "# tgt = tgt.to(device)\n",
    "# writer.add_graph(model, (src, tgt))\n",
    "# writer.close()\n",
    "# del src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7734054-af8a-4823-8b50-66f2e6c86b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.watch(model, log=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83b197ac-b9f7-46d9-be5b-a069e61a500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, optimizer, scaler, scheduler, epoch=1, use_amp=True, log_interval=10):\n",
    "    model.train()\n",
    "    model.encoder.eval()\n",
    "    losses = 0\n",
    "    with tqdm(enumerate(train_iter), total=len(train_iter), desc=f\"Epoch {epoch}\") as pbar:\n",
    "        for idx, (img, tgt) in pbar:\n",
    "            img = img.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            tgt_inp = tgt[:-1,:]      # give input until before the last word.\n",
    "            tgt_out = tgt[1:, :]      # predict the last word based on input and already predicted sentence. (auto-regressive)\n",
    "\n",
    "            tgt_mask, tgt_pad_mask = subsequent_mask(tgt_inp.size(0)), padding_mask(tgt_inp, PAD_IDX)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits = model(img, tgt_inp, tgt_mask, tgt_pad_mask)\n",
    "                loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            losses+= loss.item()\n",
    "            avg_loss = losses/(idx+1)\n",
    "            curr_lr = optimizer.param_groups[0]['lr']\n",
    "            info = {'loss': avg_loss, 'lr': curr_lr}\n",
    "\n",
    "            if not idx%log_interval:\n",
    "                wandb.log(info)\n",
    "            pbar.set_postfix(info)\n",
    "\n",
    "    return losses/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c126d-1c88-4119-a1c9-2a3cd78d4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    with tqdm(enumerate(val_iter), total=len(val_iter), desc=\"Evaluating\") as pbar:\n",
    "        for idx, (img, tgt) in pbar:\n",
    "            img = img.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            tgt_inp = tgt[:-1,:]      # give input until before the last word.\n",
    "            tgt_out = tgt[1:, :]      # predict the last word based on input and already predicted sentence. (auto-regressive)\n",
    "\n",
    "            tgt_mask, tgt_pad_mask = subsequent_mask(tgt_inp.size(0)), padding_mask(tgt_inp, PAD_IDX)\n",
    "\n",
    "            logits = model(img, tgt_inp, src_mask, tgt_mask)\n",
    "\n",
    "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "            losses+= loss.item()\n",
    "            pbar.set_postfix({'val_loss': f\"{losses/(idx+1):.3f}\"})\n",
    "    return losses/len(val_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

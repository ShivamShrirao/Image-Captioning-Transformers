{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae2eb10-f8d5-4f47-bccf-c29faf2cdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms as T\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6332628b-660d-4867-b690-e0642674c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a58c51-77fb-48f7-a6bf-52e6dcd8f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try pre trained CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39cda7a8-5913-4ff8-a871-d56f9cc16750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.utils import download_and_extract_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef7b85f2-5b20-48d0-a94a-8bfc4221309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_extract_archive(\"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "#                              download_root=\"../datasets/COCO\",\n",
    "#                              remove_finished=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4007d9-9bc7-4d75-82ee-295e43b312cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_extract_archive(\"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "#                              download_root=\"../datasets/COCO\",\n",
    "#                              remove_finished=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91386a8-b174-4720-af37-a6b2704eb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_extract_archive(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "#                              download_root=\"../datasets/COCO\",\n",
    "#                              remove_finished=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da9a4f51-0e2b-40fc-ac16-258f576b2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a236395a-fe32-4980-a822-ef25a85ce00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1bfd1dc-996b-4b19-88f0-0653ff24989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = {\n",
    "    'train': T.Compose([\n",
    "        T.RandomResizedCrop(input_size),\n",
    "        T.RandomHorizontalFlip(input_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': T.Compose([\n",
    "        T.Resize(input_size),\n",
    "        T.CenterCrop(input_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "633eb7f7-815e-42c6-9572-220d272e9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = models.resnext50_32x4d(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afc4c22d-3995-4a59-8793-008a471f6ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c58a2731-9751-47bf-a2f1-55a9c008ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, d_model=768, dp_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.LazyLinear(d_model)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dp_rate)\n",
    "        self.lin = nn.LazyLinear(d_model)\n",
    "        self.dropout2 = nn.Dropout(dp_rate)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        x = self.dropout1(self.activation(self.proj(x)))\n",
    "        x = self.dropout2(self.lin(x))\n",
    "        x = self.ln(x + inp)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bd77da0-6fa1-4898-93d2-284e362a2f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(1,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6b077a9-6aba-4f51-8a1c-a49d9c84c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.avgpool = nn.Identity() # [1, 2048, 7, 7] -> \n",
    "# flatten(x, 1)\n",
    "backbone.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ac5fccd-2f10-4058-8e2d-bce86126abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_hook(module, inp, out):\n",
    "    print(inp[0].shape)\n",
    "    print(out.shape)\n",
    "handle = backbone.avgpool.register_forward_hook(ap_hook)\n",
    "handle2 = backbone.fc.register_forward_hook(ap_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdb21928-2cb6-4767-9ce0-7db5746be3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 7, 7])\n",
      "torch.Size([1, 2048, 7, 7])\n",
      "torch.Size([1, 100352])\n",
      "torch.Size([1, 100352])\n"
     ]
    }
   ],
   "source": [
    "out = backbone(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eba97626-21c1-4be9-a36d-828a3d4677c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3efd013-4669-4a8b-96a7-da86827aa906",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.view([1, 2048, 7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7549b8fd-8eee-483f-9069-12433af8d09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 49])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(out, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ce00199-87d5-4781-a5d3-f4787c9a73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.remove()\n",
    "handle2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69003d27-7265-4a01-9aeb-07143fba2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, projection_head, tgt_vocab_size, num_decoder_layers=6, nhead=8, d_model=768,\n",
    "                 dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnext50_32x4d(pretrained=False)\n",
    "        self.freeze_weights(self.backbone)\n",
    "        self.backbone.avgpool = nn.Identity()     # \n",
    "        proj_in_dim = self.backbone.fc.in_features\n",
    "        self.backbone.fc = projection_head\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                activation)\n",
    "        decoder_norm = LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        self.generator = nn.Linear(d_model, tgt_vocab_size)\n",
    "    \n",
    "    def freeze_weights(self, model):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x, tgt):\n",
    "        x = self.backbone(x)\n",
    "        # (B, seq_len, d_model) cause batch_first.\n",
    "        x = x.permute(1,0)\n",
    "        # (seq_len, B, d_model)\n",
    "        x = x + self.pos_emb\n",
    "        x = self.decoder(tgt, memory=x, tgt_mask=tgt_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        x = self.generator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7734054-af8a-4823-8b50-66f2e6c86b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Image_Captioning_COCO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "122c00e2154e45908263f5f88e7fea3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4de778cc835748a5bca0542f9cfb7c2f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ee003f9ca6224f68b6cf372ecd7def7c",
              "IPY_MODEL_6c2341c230fe4bf09c4e4e8fcc2670cf"
            ]
          }
        },
        "4de778cc835748a5bca0542f9cfb7c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee003f9ca6224f68b6cf372ecd7def7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f518c81408ea490399dae7e7831d1c14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 252907541,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 252907541,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5a70125dcde46e396af40e188876809"
          }
        },
        "6c2341c230fe4bf09c4e4e8fcc2670cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_87a10b80bad94173a6a8aed5a205d63b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 252908544/? [00:10&lt;00:00, 24168690.21it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11981c3100f64363aac2237ab2438cf2"
          }
        },
        "f518c81408ea490399dae7e7831d1c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5a70125dcde46e396af40e188876809": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87a10b80bad94173a6a8aed5a205d63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11981c3100f64363aac2237ab2438cf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7840ce0e-dee1-4323-9eeb-05fda2898d8f"
      },
      "source": [
        "# Image Captioning with Transformers"
      ],
      "id": "7840ce0e-dee1-4323-9eeb-05fda2898d8f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGijTnwSnnIM",
        "outputId": "170a06c5-a957-43cb-d226-7d6cd3a46297"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "qGijTnwSnnIM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 27 02:53:52 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGt3Q64Zogx2",
        "outputId": "51ae7047-88a4-45cc-9dc9-d5ae650a8e03"
      },
      "source": [
        "!apt install -qq pigz\n",
        "%pip install -q timm wandb\n",
        "%pip install -q --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110"
      ],
      "id": "VGt3Q64Zogx2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following NEW packages will be installed:\n",
            "  pigz\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 57.4 kB of archives.\n",
            "After this operation, 259 kB of additional disk space will be used.\n",
            "Selecting previously unselected package pigz.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/pigz_2.4-1_amd64.deb ...\n",
            "Unpacking pigz (2.4-1) ...\n",
            "Setting up pigz (2.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "\u001b[K     |████████████████████████████████| 348kB 5.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 21.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 20.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.8MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 613.6MB 24kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujdZEDZJnpe3",
        "outputId": "40bdf9a6-b73f-4620-bee2-963c60614184"
      },
      "source": [
        "!git clone https://github.com/ShivamShrirao/Image-Captioning-Transformers"
      ],
      "id": "ujdZEDZJnpe3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Image-Captioning-Transformers'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 46 (delta 16), reused 38 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "819264fd-bf0c-4862-9dbd-c4d2504a7071"
      },
      "source": [
        "# Download Dataset and Annotations"
      ],
      "id": "819264fd-bf0c-4862-9dbd-c4d2504a7071"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKeB5Z8UYU8A"
      },
      "source": [
        "!mkdir ~/.kaggle/\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "cKeB5Z8UYU8A",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IOQUsLEHD2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4d2597-73ac-48ff-f7ba-435fc051d87b"
      },
      "source": [
        "!kaggle datasets download -d shivamshrirao/coco-trainval2017-320x320"
      ],
      "id": "9IOQUsLEHD2z",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading coco-trainval2017-320x320.zip to /content\n",
            "100% 3.45G/3.46G [00:37<00:00, 108MB/s]\n",
            "100% 3.46G/3.46G [00:37<00:00, 100MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCLNX6TaYt4d"
      },
      "source": [
        "!unzip -q coco-trainval2017-320x320.zip"
      ],
      "id": "vCLNX6TaYt4d",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veRLNXL-xtYR"
      },
      "source": [
        "# !gdown --id 1-3vdwBlY-CdVultkrFwOhyJTGC5TFUV8"
      ],
      "id": "veRLNXL-xtYR",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbpjO-f9wWQt"
      },
      "source": [
        "# !pigz -dc coco_trainval2017_320x320.tar.gz | tar xf -"
      ],
      "id": "WbpjO-f9wWQt",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39cda7a8-5913-4ff8-a871-d56f9cc16750"
      },
      "source": [
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "DATA_DIR = \"datasets/COCO\""
      ],
      "id": "39cda7a8-5913-4ff8-a871-d56f9cc16750",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "122c00e2154e45908263f5f88e7fea3f",
            "4de778cc835748a5bca0542f9cfb7c2f",
            "ee003f9ca6224f68b6cf372ecd7def7c",
            "6c2341c230fe4bf09c4e4e8fcc2670cf",
            "f518c81408ea490399dae7e7831d1c14",
            "e5a70125dcde46e396af40e188876809",
            "87a10b80bad94173a6a8aed5a205d63b",
            "11981c3100f64363aac2237ab2438cf2"
          ]
        },
        "id": "e91386a8-b174-4720-af37-a6b2704eb269",
        "outputId": "1ba38928-906a-4160-8a95-12d740b9d574"
      },
      "source": [
        "download_and_extract_archive(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
        "                             download_root=DATA_DIR,\n",
        "                             remove_finished=True)"
      ],
      "id": "e91386a8-b174-4720-af37-a6b2704eb269",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://images.cocodataset.org/annotations/annotations_trainval2017.zip to datasets/COCO/annotations_trainval2017.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "122c00e2154e45908263f5f88e7fea3f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=252907541.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting datasets/COCO/annotations_trainval2017.zip to datasets/COCO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWKDs4aIux4R"
      },
      "source": [
        "!rm coco-trainval2017-320x320.* datasets/COCO/annotations_trainval2017.zip"
      ],
      "id": "qWKDs4aIux4R",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcFgDzJP8MO4"
      },
      "source": [
        "# Import libraries"
      ],
      "id": "zcFgDzJP8MO4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW7EJRTVnwll",
        "outputId": "9293a863-3ea4-4071-fef1-2625b93b63cf"
      },
      "source": [
        "%cd Image-Captioning-Transformers"
      ],
      "id": "qW7EJRTVnwll",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Image-Captioning-Transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ffdef7-f21f-4450-ae9a-c43b7f79cba0"
      },
      "source": [
        "#hide\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "id": "07ffdef7-f21f-4450-ae9a-c43b7f79cba0",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98a58c51-77fb-48f7-a6bf-52e6dcd8f93a"
      },
      "source": [
        "# TODO: Try pre trained CLIP"
      ],
      "id": "98a58c51-77fb-48f7-a6bf-52e6dcd8f93a",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aae2eb10-f8d5-4f47-bccf-c29faf2cdbcd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms as T"
      ],
      "id": "aae2eb10-f8d5-4f47-bccf-c29faf2cdbcd",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6332628b-660d-4867-b690-e0642674c4ac"
      },
      "source": [
        "import math\n",
        "import random\n",
        "from random import randint\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict"
      ],
      "id": "6332628b-660d-4867-b690-e0642674c4ac",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcb4177c-640f-469c-9ac4-d09a19ee8e37"
      },
      "source": [
        "import timm"
      ],
      "id": "fcb4177c-640f-469c-9ac4-d09a19ee8e37",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc18b9ba-75cb-4b7a-a909-cb226522e981"
      },
      "source": [
        "plt.rcParams['figure.facecolor'] = 'white'"
      ],
      "id": "dc18b9ba-75cb-4b7a-a909-cb226522e981",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f978fc3-2bca-40cf-af8a-263371f97577"
      },
      "source": [
        "# Wandb Parameters"
      ],
      "id": "2f978fc3-2bca-40cf-af8a-263371f97577"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6f8f439-401b-4dd3-ada8-fbbfcf2beded"
      },
      "source": [
        "import wandb"
      ],
      "id": "e6f8f439-401b-4dd3-ada8-fbbfcf2beded",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b59efe92-566d-46ab-a91a-6323a855ff40"
      },
      "source": [
        "config_defaults = {\n",
        "    'BATCH_SIZE'        : 256,\n",
        "    'd_model'           : 512,\n",
        "    'dim_feedforward'   : 1024,\n",
        "    'nheads'            : 8,\n",
        "    'num_decoder_layers': 6,\n",
        "    'dp_rate'           : 0.1,\n",
        "    'encoder'           : 'seresnext50_32x4d',\n",
        "    'activation'        : 'gelu',\n",
        "    'max_lr'            : 6e-4,\n",
        "    'betas'             : (0.9, 0.98),\n",
        "    'eps'               : 1e-9,\n",
        "    'seed'              : 62134,\n",
        "    'use_amp'           : True,\n",
        "    'use_pe'            : True,\n",
        "    'log_interval'      : 5,\n",
        "}\n",
        "CONFIG = config_defaults"
      ],
      "id": "b59efe92-566d-46ab-a91a-6323a855ff40",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b5aca5c-bf25-45b8-b3e3-73652fa09a3d"
      },
      "source": [
        "# #hide\n",
        "# run = wandb.init(id='xjgc55j0', project=\"Image_Captioning_Transformer\", resume='must')\n",
        "# CONFIG = run.config"
      ],
      "id": "3b5aca5c-bf25-45b8-b3e3-73652fa09a3d",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09f936c7-fd7f-42f6-89d9-d0b4468fb48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "670b4836-9ec3-45fe-d33d-5284dfecda42"
      },
      "source": [
        "run = wandb.init(project=\"Image_Captioning_Transformer\", entity=\"shivamshrirao\", config=config_defaults)\n",
        "CONFIG = wandb.config"
      ],
      "id": "09f936c7-fd7f-42f6-89d9-d0b4468fb48c",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.32<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">youthful-shape-18</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/shivamshrirao/Image_Captioning_Transformer\" target=\"_blank\">https://wandb.ai/shivamshrirao/Image_Captioning_Transformer</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/shivamshrirao/Image_Captioning_Transformer/runs/1rudn15w\" target=\"_blank\">https://wandb.ai/shivamshrirao/Image_Captioning_Transformer/runs/1rudn15w</a><br/>\n",
              "                Run data is saved locally in <code>/content/Image-Captioning-Transformers/wandb/run-20210627_025716-1rudn15w</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mfbg5gKZ8-E"
      },
      "source": [
        "def seed_everything(seed=33):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(CONFIG['seed'])"
      ],
      "id": "_mfbg5gKZ8-E",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a45cb594-a2d0-477c-8df2-007a76c106dc"
      },
      "source": [
        "# Preprocessing Transforms"
      ],
      "id": "a45cb594-a2d0-477c-8df2-007a76c106dc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da9a4f51-0e2b-40fc-ac16-258f576b2794"
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "da9a4f51-0e2b-40fc-ac16-258f576b2794",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a236395a-fe32-4980-a822-ef25a85ce00f"
      },
      "source": [
        "input_size = 224"
      ],
      "id": "a236395a-fe32-4980-a822-ef25a85ce00f",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1bfd1dc-996b-4b19-88f0-0653ff24989a"
      },
      "source": [
        "# first transform crop while loading, then do rest later in batch on device\n",
        "preproc = {\n",
        "    'train': T.Compose([\n",
        "        T.RandomResizedCrop(input_size, interpolation=T.InterpolationMode.BICUBIC),\n",
        "        T.RandomHorizontalFlip(input_size),\n",
        "        lambda image: image.convert(\"RGB\"),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'val': T.Compose([\n",
        "        T.Resize(input_size, interpolation=T.InterpolationMode.BICUBIC),\n",
        "        T.CenterCrop(input_size),\n",
        "        lambda image: image.convert(\"RGB\"),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "id": "f1bfd1dc-996b-4b19-88f0-0653ff24989a",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e2f199d-ba6f-4d12-a3b0-76a5f987f0c9"
      },
      "source": [
        "## Read COCO dataset"
      ],
      "id": "9e2f199d-ba6f-4d12-a3b0-76a5f987f0c9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5914e791-b1fd-405b-aa65-bdd1d9e13e37"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchvision.io import read_file, decode_jpeg, ImageReadMode\n",
        "import torchtext\n",
        "import os"
      ],
      "id": "5914e791-b1fd-405b-aa65-bdd1d9e13e37",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_20KeTj8rk0"
      },
      "source": [
        "DATA_DIR = \"../datasets/COCO/\""
      ],
      "id": "G_20KeTj8rk0",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX4zo7vW_Enk"
      },
      "source": [
        "class TensorCocoCaptions(datasets.CocoDetection):\n",
        "    def __getitem__(self, index: int):\n",
        "        return super().__getitem__(index % len(self.ids))\n",
        "    \n",
        "    def _load_target(self, id):\n",
        "        return self.tokens_dict[id]\n",
        "    \n",
        "    def fill_token_dict(self, tokenizer, vocab, bos_idx, eos_idx):\n",
        "        self.tokens_dict = {}                       # To save preprocessed captions as tokens.\n",
        "        for id in tqdm(self.ids):\n",
        "            captions = self._load_caption(id)\n",
        "            self.tokens_dict[id] = [torch.tensor([bos_idx] + vocab(tokenizer(cap)) + [eos_idx]#, dtype=torch.int32)\n",
        "                                    for cap in captions]\n",
        "    \n",
        "    def _load_caption(self, id):\n",
        "        return [ann[\"caption\"] for ann in super()._load_target(id)]\n",
        "\n",
        "    def _load_image(self, id):\n",
        "        path = self.coco.loadImgs(id)[0][\"file_name\"]\n",
        "        data = read_file(os.path.join(self.root, path))\n",
        "        return data\n",
        "        # return decode_jpeg(data, ImageReadMode.RGB)#, device=DEVICE)"
      ],
      "id": "dX4zo7vW_Enk",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ca3492-26a4-4310-bec7-44cec5092a62",
        "outputId": "6a6f297a-aa33-4413-9770-5f314b50804b"
      },
      "source": [
        "train_data = TensorCocoCaptions(root=DATA_DIR+\"/train2017/\",\n",
        "                                annFile=DATA_DIR+\"/annotations/captions_train2017.json\")\n",
        "\n",
        "val_data = TensorCocoCaptions(root=DATA_DIR+\"/val2017/\",\n",
        "                              annFile=DATA_DIR+\"/annotations/captions_val2017.json\")"
      ],
      "id": "96ca3492-26a4-4310-bec7-44cec5092a62",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.07s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.06s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93d1c495-f3d0-4b58-bb88-286530849969"
      },
      "source": [
        "## Tokenizer and Build Vocab"
      ],
      "id": "93d1c495-f3d0-4b58-bb88-286530849969"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4e10704-bd29-465c-8626-b0a48ee2f01b"
      },
      "source": [
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "id": "b4e10704-bd29-465c-8626-b0a48ee2f01b",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c84dd7aa-6686-4e2f-8537-919460d0a12c"
      },
      "source": [
        "def yield_tokens(cap_data):\n",
        "    for ann in cap_data.coco.anns.values():\n",
        "        yield tokenizer(ann['caption'])"
      ],
      "id": "c84dd7aa-6686-4e2f-8537-919460d0a12c",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8922a7a-5747-4a04-95fc-976a6947a5ba"
      },
      "source": [
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "en_vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=special_symbols, special_first=True)\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = en_vocab(special_symbols)\n",
        "en_vocab.set_default_index(UNK_IDX)"
      ],
      "id": "f8922a7a-5747-4a04-95fc-976a6947a5ba",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRJA2NEq52Rh",
        "outputId": "227d6e6e-fc2c-4f7c-8c80-3e9f82618260"
      },
      "source": [
        "len(en_vocab)"
      ],
      "id": "wRJA2NEq52Rh",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28940"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MFKWGiOB9K5",
        "outputId": "de4d46bf-1fbd-4ce0-85a6-6a53aa127d8b"
      },
      "source": [
        "train_data.fill_token_dict(tokenizer, en_vocab, BOS_IDX, EOS_IDX)\n",
        "val_data.fill_token_dict(tokenizer, en_vocab, BOS_IDX, EOS_IDX)"
      ],
      "id": "-MFKWGiOB9K5",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 118287/118287 [00:22<00:00, 5319.52it/s]\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 5530.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d493c53-ce30-443e-936e-a1dac5270477"
      },
      "source": [
        "## Pretrained Glove Embeddings (not used rn)"
      ],
      "id": "6d493c53-ce30-443e-936e-a1dac5270477"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "042950b8-a6d3-494d-997f-1edbc2e53339"
      },
      "source": [
        "# vec.get_vecs_by_tokens(tokens, lower_case_backup=True)"
      ],
      "id": "042950b8-a6d3-494d-997f-1edbc2e53339",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "893f50d5-adff-459f-a09c-10e001e48d23"
      },
      "source": [
        "# vec = torchtext.vocab.GloVe('6B', dim=300)\n",
        "# unk_vec = vec.vectors.mean(dim=0)\n",
        "# vec.unk_init = lambda x: unk_vec"
      ],
      "id": "893f50d5-adff-459f-a09c-10e001e48d23",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a7920ad-38a9-4fc7-9878-a0765ddac8fd"
      },
      "source": [
        "# Load dataset into batches"
      ],
      "id": "8a7920ad-38a9-4fc7-9878-a0765ddac8fd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUyM5TOgAQ8t"
      },
      "source": [
        "from nvidia.dali.pipeline import Pipeline\n",
        "import nvidia.dali.fn as fn\n",
        "import nvidia.dali.types as types\n",
        "from nvidia.dali.plugin.pytorch import DALIClassificationIterator\n",
        "from random import shuffle"
      ],
      "id": "YUyM5TOgAQ8t",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29m0-RUqRI1v"
      },
      "source": [
        "class ExternalInputIterator(object):\n",
        "    def __init__(self, dataset, batch_size, training=True):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.training = training\n",
        "        if self.training: shuffle(train_data.ids)\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.idx = 0\n",
        "        if self.training: shuffle(train_data.ids)\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        img_batch = []\n",
        "        cap_batch = []\n",
        "\n",
        "        if self.idx >= len(self.dataset):\n",
        "            self.__iter__()\n",
        "            raise StopIteration\n",
        "\n",
        "        for _ in range(self.batch_size):\n",
        "            img, caps = self.dataset[self.idx]\n",
        "            img_batch.append(img)\n",
        "            cap = caps[randint(0,len(caps)-1) if self.training else 0]\n",
        "            cap_batch.append(cap)\n",
        "            self.idx += 1\n",
        "        cap_batch = pad_sequence(cap_batch, batch_first=True, padding_value=PAD_IDX)#.type(torch.long)\n",
        "        return (img_batch, cap_batch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    next = __next__"
      ],
      "id": "29m0-RUqRI1v",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aazPtQVh2Aw"
      },
      "source": [
        "def ExternalSourcePipeline(batch_size, num_threads, device_id, external_data, training=True):\n",
        "    pipe = Pipeline(batch_size, num_threads, device_id)\n",
        "    with pipe:\n",
        "        images, labels = fn.external_source(source=external_data, num_outputs=2)\n",
        "        if training:\n",
        "            images = fn.decoders.image_random_crop(images, device='mixed', output_type=types.RGB, num_attempts=100, memory_stats=True)\n",
        "            mirror = fn.random.coin_flip(probability=0.5)\n",
        "        else:\n",
        "            images = fn.decoders.image(images, device='mixed', output_type=types.RGB)\n",
        "            mirror = False\n",
        "        images = fn.resize(images, device='gpu', resize_shorter=input_size, interp_type=types.INTERP_TRIANGULAR)\n",
        "        images = fn.crop_mirror_normalize(images.gpu(),\n",
        "                                          dtype=types.FLOAT,\n",
        "                                          output_layout=\"CHW\",\n",
        "                                          crop=(input_size, input_size),\n",
        "                                          mean=[0.485 * 255,0.456 * 255,0.406 * 255],\n",
        "                                          std=[0.229 * 255,0.224 * 255,0.225 * 255],\n",
        "                                          mirror=mirror)\n",
        "        labels = labels.gpu()\n",
        "        pipe.set_outputs(images, labels)\n",
        "    return pipe"
      ],
      "id": "9aazPtQVh2Aw",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puscj06UBKJl"
      },
      "source": [
        "train_iter = ExternalInputIterator(train_data, CONFIG['BATCH_SIZE'])\n",
        "pipe = ExternalSourcePipeline(batch_size=CONFIG['BATCH_SIZE'], num_threads=4, device_id=0, external_data=train_iter)\n",
        "train_loader = DALIClassificationIterator(pipe, dynamic_shape=True, auto_reset=True, last_batch_padded=True, size=len(train_iter))\n",
        "\n",
        "val_iter = ExternalInputIterator(val_data, CONFIG['BATCH_SIZE'], training=False)\n",
        "pipe = ExternalSourcePipeline(batch_size=CONFIG['BATCH_SIZE'], num_threads=4, device_id=0, external_data=val_iter, training=False)\n",
        "val_loader = DALIClassificationIterator(pipe, dynamic_shape=True, auto_reset=True, last_batch_padded=True, size=len(val_iter))"
      ],
      "id": "puscj06UBKJl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0c04da7-ebc6-4ea8-b803-e5529df1668b"
      },
      "source": [
        "# Initialize Model"
      ],
      "id": "d0c04da7-ebc6-4ea8-b803-e5529df1668b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c58a2731-9751-47bf-a2f1-55a9c008ac33"
      },
      "source": [
        "from imcap.layers import *\n",
        "from imcap.utils import *"
      ],
      "id": "c58a2731-9751-47bf-a2f1-55a9c008ac33",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c54e00a1-b9e4-4841-befe-6b55ec0f75fd"
      },
      "source": [
        "model = CaptionModel(encoder = timm.create_model(CONFIG['encoder'], pretrained=True, num_classes=0, global_pool=''),\n",
        "                     vocab_size = len(en_vocab),\n",
        "                     num_decoder_layers = CONFIG['num_decoder_layers'],\n",
        "                     nheads = CONFIG['nheads'],\n",
        "                     d_model = CONFIG['d_model'],\n",
        "                     dim_feedforward = CONFIG['dim_feedforward'],\n",
        "                     dp_rate = CONFIG['dp_rate'],\n",
        "                     activation = CONFIG['activation']).to(DEVICE, non_blocking=True)"
      ],
      "id": "c54e00a1-b9e4-4841-befe-6b55ec0f75fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08b4b6bf-c036-429f-b5e5-1d053f3d91e9"
      },
      "source": [
        "# Learning Rate Schedule"
      ],
      "id": "08b4b6bf-c036-429f-b5e5-1d053f3d91e9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5639c04c-f3f9-4c28-8bff-cdd7bc2fc02a"
      },
      "source": [
        "steps_per_epoch = len(train_loader)"
      ],
      "id": "5639c04c-f3f9-4c28-8bff-cdd7bc2fc02a",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6336bdd7-aee5-4bd7-b382-cb18e023aa8f"
      },
      "source": [
        "# def lr_schedule(step, d_model=512, warmup_steps=2*steps_per_epoch):\n",
        "#     # return 1\n",
        "#     step = max(1,step)\n",
        "#     arg1 = step ** -0.5\n",
        "#     arg2 = step * (warmup_steps ** -1.5)\n",
        "#     return (d_model ** -0.6) * min(arg1, arg2)"
      ],
      "id": "6336bdd7-aee5-4bd7-b382-cb18e023aa8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBR3qr5gd3SI"
      },
      "source": [
        "# plt.plot([scheduler.get_last_lr()[0] for _ in range(steps_per_epoch*50) if not scheduler.step()])\n",
        "# plt.show()"
      ],
      "id": "PBR3qr5gd3SI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceca2478-81ff-402f-b984-72c19e0391d0"
      },
      "source": [
        "# plt.plot(list(map(lr_schedule, range(steps_per_epoch*50))))\n",
        "# plt.show()"
      ],
      "id": "ceca2478-81ff-402f-b984-72c19e0391d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc37d126-9abb-4a3e-8304-cd589dee058e"
      },
      "source": [
        "# Loss Function and Optimizer"
      ],
      "id": "dc37d126-9abb-4a3e-8304-cd589dee058e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "530023ee-432b-4ed8-ba6d-1831dffaebf6"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=CONFIG['max_lr'], betas=CONFIG['betas'], eps=CONFIG['eps']\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CONFIG['max_lr'], total_steps=50*steps_per_epoch, pct_start=0.04, final_div_factor=0.31)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=CONFIG['use_amp'])"
      ],
      "id": "530023ee-432b-4ed8-ba6d-1831dffaebf6",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7734054-af8a-4823-8b50-66f2e6c86b97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e548cf04-f541-4bee-fe28-f4bbf8a0e84a"
      },
      "source": [
        "wandb.watch(model, log=None)"
      ],
      "id": "f7734054-af8a-4823-8b50-66f2e6c86b97",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<wandb.wandb_torch.TorchGraph at 0x7f04b452bc50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTJuKcv07aBa"
      },
      "source": [
        "# Training functions"
      ],
      "id": "dTJuKcv07aBa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95a5da74-3381-46df-aeb0-3d2f6632d97a"
      },
      "source": [
        "from torch.cuda import amp"
      ],
      "id": "95a5da74-3381-46df-aeb0-3d2f6632d97a",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83b197ac-b9f7-46d9-be5b-a069e61a500b"
      },
      "source": [
        "def train_epoch(model, train_loader, optimizer, scaler, scheduler, epoch=1, use_amp=True, log_interval=10):\n",
        "    model.train()\n",
        "    model.encoder.eval()\n",
        "    losses = 0\n",
        "    with tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\") as pbar:\n",
        "        for idx, batch in pbar:\n",
        "            img, tgt = batch[0]['data'], batch[0]['label'].transpose(0,1)\n",
        "            # img = img.to(DEVICE, non_blocking=True)\n",
        "            # tgt = tgt.to(DEVICE, non_blocking=True)\n",
        "            \n",
        "            tgt_inp = tgt[:-1,:]      # give input until before the last word.\n",
        "            tgt_out = tgt[1:, :]      # predict the last word based on input and already predicted sentence. (auto-regressive)\n",
        "\n",
        "            tgt_mask, tgt_pad_mask = subsequent_mask(tgt_inp.size(0), DEVICE), padding_mask(tgt_inp, PAD_IDX)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with amp.autocast(enabled=use_amp):\n",
        "                logits = model(img, tgt_inp, tgt_mask, tgt_pad_mask)\n",
        "                loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            losses+= loss.detach_()\n",
        "            del loss, logits, batch, img\n",
        "\n",
        "            if not idx%log_interval:\n",
        "                curr_lr = optimizer.param_groups[0]['lr']\n",
        "                losses = float(losses)\n",
        "                info = {'loss': losses/(idx+1), 'lr': curr_lr}\n",
        "                wandb.log(info)\n",
        "                pbar.set_postfix(info)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    return float(losses)/len(train_loader)"
      ],
      "id": "83b197ac-b9f7-46d9-be5b-a069e61a500b",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06c126d-1c88-4119-a1c9-2a3cd78d4e9b"
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader, use_amp=True):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    with tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Evaluating\") as pbar:\n",
        "        for idx, batch in pbar:\n",
        "            img, tgt = batch[0]['data'], batch[0]['label'].transpose(0,1)\n",
        "            # img = img.to(DEVICE, non_blocking=True)\n",
        "            # tgt = tgt.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            tgt_inp = tgt[:-1,:]      # give input until before the last word.\n",
        "            tgt_out = tgt[1:, :]      # predict the last word based on input and already predicted sentence. (auto-regressive)\n",
        "\n",
        "            tgt_mask, tgt_pad_mask = subsequent_mask(tgt_inp.size(0), DEVICE), padding_mask(tgt_inp, PAD_IDX)\n",
        "            \n",
        "            with amp.autocast(enabled=use_amp):\n",
        "                logits = model(img, tgt_inp, tgt_mask, tgt_pad_mask)\n",
        "                loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "\n",
        "            losses+= float(loss.detach_())\n",
        "            pbar.set_postfix({'val_loss': losses/(idx+1)})\n",
        "    return float(losses)/len(val_loader)"
      ],
      "id": "c06c126d-1c88-4119-a1c9-2a3cd78d4e9b",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f954d0ba-cc4e-4b9d-bec3-202183c517ca"
      },
      "source": [
        "# Functions to Make Predictions"
      ],
      "id": "f954d0ba-cc4e-4b9d-bec3-202183c517ca"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34341b74-edb8-4e7c-8f05-43b3c0f0db2a"
      },
      "source": [
        "@torch.no_grad()\n",
        "def greedy_decode(model, img, max_len=100, start_symbol=BOS_IDX):\n",
        "    model.eval()\n",
        "    img = img.to(DEVICE, non_blocking=True)\n",
        "    enc_output = model.encode_image(img)\n",
        "    tgt = torch.ones(1, 1).fill_(start_symbol).long().to(DEVICE, non_blocking=True)\n",
        "    for i in range(max_len):\n",
        "        tgt_mask = subsequent_mask(tgt.size(0), DEVICE)\n",
        "        out = model.decode_text(tgt, enc_output, tgt_mask)\n",
        "        out = out.transpose(0,1)\n",
        "        prob = model.generator(out[:,-1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "        tgt = torch.cat([tgt, torch.ones(1, 1).fill_(next_word).long().to(DEVICE)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return tgt.detach()\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_caption(model, img, tgt_vocab):\n",
        "    tgt = greedy_decode(model, img, max_len=100, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(tgt_vocab.lookup_tokens(tgt.tolist())).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "id": "34341b74-edb8-4e7c-8f05-43b3c0f0db2a",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGEcLDCH7g7E"
      },
      "source": [
        "# Begin Training"
      ],
      "id": "GGEcLDCH7g7E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53012613-4bdd-4e1f-abe1-e9c4ee9fa3d3"
      },
      "source": [
        "init_epoch = 1\n",
        "NUM_EPOCHS = 50"
      ],
      "id": "53012613-4bdd-4e1f-abe1-e9c4ee9fa3d3",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb9476c9-376f-49f5-a3bd-881210d50d0d"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "id": "cb9476c9-376f-49f5-a3bd-881210d50d0d",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4391b037-7a0a-4179-82e9-d7e9e1ff6c11"
      },
      "source": [
        "import glob\n",
        "val_paths = glob.glob(\"../datasets/COCO/val2017/*\")"
      ],
      "id": "4391b037-7a0a-4179-82e9-d7e9e1ff6c11",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwpPA5d7o7bK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d012990-4247-41f4-e74b-9174a308d612"
      },
      "source": [
        "#collapse-output\n",
        "for epoch in range(init_epoch, NUM_EPOCHS+1):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, scaler, scheduler,\n",
        "                             epoch, CONFIG['use_amp'], CONFIG['log_interval'])\n",
        "    with torch.no_grad():\n",
        "        val_loss = evaluate(model, val_loader, CONFIG['use_amp'])\n",
        "\n",
        "        img = Image.open(random.choice(val_paths))\n",
        "        caps = generate_caption(model, preproc['val'](img)[None,:], en_vocab)\n",
        "        wandb.log({\"val_loss\": val_loss, \"epoch\": epoch, \"examples\": wandb.Image(img, caption=caps)})\n",
        "        print(f\"\\nEpoch: {epoch}/{NUM_EPOCHS}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\\n\")\n",
        "    # gc.collect()\n",
        "    # if not epoch%10:\n",
        "    #     save_model(model, optimizer, epoch)"
      ],
      "id": "qwpPA5d7o7bK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 463/463 [06:33<00:00,  1.18it/s, loss=4.95, lr=0.000311]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.9450, device='cuda:0')]\n",
            "Epoch 2:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/50, Train loss: 4.942, Val loss: 2.945\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 463/463 [06:28<00:00,  1.19it/s, loss=2.92, lr=0.0006]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.4849, device='cuda:0')]\n",
            "Epoch 3:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2/50, Train loss: 2.921, Val loss: 2.485\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 463/463 [06:24<00:00,  1.20it/s, loss=2.65, lr=0.000599]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.3640, device='cuda:0')]\n",
            "Epoch 4:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3/50, Train loss: 2.650, Val loss: 2.364\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 463/463 [06:29<00:00,  1.19it/s, loss=2.55, lr=0.000598]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.2774, device='cuda:0')]\n",
            "Epoch 5:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 4/50, Train loss: 2.545, Val loss: 2.277\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 463/463 [06:27<00:00,  1.19it/s, loss=2.48, lr=0.000595]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.2343, device='cuda:0')]\n",
            "Epoch 6:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 5/50, Train loss: 2.479, Val loss: 2.234\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 463/463 [06:29<00:00,  1.19it/s, loss=2.42, lr=0.000591]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.2035, device='cuda:0')]\n",
            "Epoch 7:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 6/50, Train loss: 2.425, Val loss: 2.204\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 463/463 [06:28<00:00,  1.19it/s, loss=2.4, lr=0.000586]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.1791, device='cuda:0')]\n",
            "Epoch 8:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 7/50, Train loss: 2.395, Val loss: 2.179\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 463/463 [06:29<00:00,  1.19it/s, loss=2.36, lr=0.00058]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.1588, device='cuda:0')]\n",
            "Epoch 9:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 8/50, Train loss: 2.363, Val loss: 2.159\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 463/463 [06:27<00:00,  1.20it/s, loss=2.34, lr=0.000573]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.57it/s, val_loss=tensor(2.1302, device='cuda:0')]\n",
            "Epoch 10:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 9/50, Train loss: 2.337, Val loss: 2.130\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 463/463 [06:26<00:00,  1.20it/s, loss=2.31, lr=0.000565]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.1230, device='cuda:0')]\n",
            "Epoch 11:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 10/50, Train loss: 2.314, Val loss: 2.123\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|██████████| 463/463 [06:29<00:00,  1.19it/s, loss=2.3, lr=0.000556]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.57it/s, val_loss=tensor(2.1109, device='cuda:0')]\n",
            "Epoch 12:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 11/50, Train loss: 2.295, Val loss: 2.111\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|██████████| 463/463 [06:27<00:00,  1.20it/s, loss=2.28, lr=0.000546]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.0954, device='cuda:0')]\n",
            "Epoch 13:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 12/50, Train loss: 2.279, Val loss: 2.095\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|██████████| 463/463 [06:28<00:00,  1.19it/s, loss=2.26, lr=0.000535]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.0877, device='cuda:0')]\n",
            "Epoch 14:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 13/50, Train loss: 2.263, Val loss: 2.088\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|██████████| 463/463 [06:28<00:00,  1.19it/s, loss=2.25, lr=0.000523]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.0774, device='cuda:0')]\n",
            "Epoch 15:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 14/50, Train loss: 2.248, Val loss: 2.077\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|██████████| 463/463 [06:29<00:00,  1.19it/s, loss=2.24, lr=0.000511]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.0666, device='cuda:0')]\n",
            "Epoch 16:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 15/50, Train loss: 2.236, Val loss: 2.067\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|██████████| 463/463 [06:29<00:00,  1.19it/s, loss=2.22, lr=0.000498]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.0582, device='cuda:0')]\n",
            "Epoch 17:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 16/50, Train loss: 2.221, Val loss: 2.058\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|██████████| 463/463 [06:29<00:00,  1.19it/s, loss=2.2, lr=0.000484]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.0500, device='cuda:0')]\n",
            "Epoch 18:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 17/50, Train loss: 2.203, Val loss: 2.050\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|██████████| 463/463 [06:27<00:00,  1.19it/s, loss=2.19, lr=0.000469]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.0426, device='cuda:0')]\n",
            "Epoch 19:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 18/50, Train loss: 2.190, Val loss: 2.043\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|██████████| 463/463 [06:29<00:00,  1.19it/s, loss=2.18, lr=0.000454]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.0358, device='cuda:0')]\n",
            "Epoch 20:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 19/50, Train loss: 2.181, Val loss: 2.036\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|██████████| 463/463 [06:28<00:00,  1.19it/s, loss=2.17, lr=0.000439]\n",
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, val_loss=tensor(2.0282, device='cuda:0')]\n",
            "Epoch 21:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 20/50, Train loss: 2.171, Val loss: 2.028\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21:  12%|█▏        | 55/463 [00:47<05:44,  1.18it/s, loss=2.15, lr=0.000437]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0694d7b8-8858-40c4-9aa5-06af829377f9"
      },
      "source": [
        "init_epoch = epoch\n",
        "init_epoch"
      ],
      "id": "0694d7b8-8858-40c4-9aa5-06af829377f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0a69310-dc2b-4bf5-a8e4-35865105342c"
      },
      "source": [
        "# def save_model(model, optimizer, epoch):\n",
        "#     torch.save({\n",
        "#                 'model_state_dict': model.state_dict(),\n",
        "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                 'epoch': epoch,\n",
        "#                 }, '/content/model.pth')"
      ],
      "id": "e0a69310-dc2b-4bf5-a8e4-35865105342c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFfXPsVtLai7"
      },
      "source": [
        "# Make Predictions"
      ],
      "id": "zFfXPsVtLai7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI44iCZFKLKH"
      },
      "source": [
        "img = Image.open(random.choice(val_paths))\n",
        "caps = generate_caption(model, preproc['val'](img)[None,:], en_vocab)\n",
        "# wandb.log({\"examples\": wandb.Image(img, caption=caps)})\n",
        "print(caps)\n",
        "img"
      ],
      "id": "AI44iCZFKLKH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17db2ed5-573d-4926-8567-3637b42338ae"
      },
      "source": [
        "run.finish()"
      ],
      "id": "17db2ed5-573d-4926-8567-3637b42338ae",
      "execution_count": null,
      "outputs": []
    }
  ]
}
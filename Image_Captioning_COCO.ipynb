{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Image_Captioning_COCO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0898b33c61064564add9813b3a7a1978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_056252a2d78945ed974a03cccf1bce37",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_032a94865b6d47e097aca3c8feae040b",
              "IPY_MODEL_8f54523326894828b41674e84aafdcae"
            ]
          }
        },
        "056252a2d78945ed974a03cccf1bce37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "032a94865b6d47e097aca3c8feae040b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_65e1f173865d49088a7d60353b102eb4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 252907541,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 252907541,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e0d62b01afd41109863898faf1b42ed"
          }
        },
        "8f54523326894828b41674e84aafdcae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c81d5fb60b2c4cf281657796c4a7e0e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 252908544/? [03:52&lt;00:00, 1089187.26it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_87372d37c5404861aac5018f51c9e2e6"
          }
        },
        "65e1f173865d49088a7d60353b102eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e0d62b01afd41109863898faf1b42ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c81d5fb60b2c4cf281657796c4a7e0e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "87372d37c5404861aac5018f51c9e2e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7840ce0e-dee1-4323-9eeb-05fda2898d8f"
      },
      "source": [
        "# Image Captioning with Transformers"
      ],
      "id": "7840ce0e-dee1-4323-9eeb-05fda2898d8f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGijTnwSnnIM",
        "outputId": "a51af366-ad4c-424d-f940-5e2f44885114"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "qGijTnwSnnIM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 27 19:26:32 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGt3Q64Zogx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d1e289-2d7a-4dcc-c621-033c32ad85d9"
      },
      "source": [
        "!apt install -qq pigz\n",
        "%pip install -q timm wandb\n",
        "%pip install -q --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110"
      ],
      "id": "VGt3Q64Zogx2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following NEW packages will be installed:\n",
            "  pigz\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 57.4 kB of archives.\n",
            "After this operation, 259 kB of additional disk space will be used.\n",
            "Selecting previously unselected package pigz.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/pigz_2.4-1_amd64.deb ...\n",
            "Unpacking pigz (2.4-1) ...\n",
            "Setting up pigz (2.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "\u001b[K     |████████████████████████████████| 348kB 29.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 40.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 45.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 52.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 12.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 12.0MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 613.6MB 26kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujdZEDZJnpe3",
        "outputId": "2eaad42e-bbbd-4c6b-8441-87f2203c30d1"
      },
      "source": [
        "!git clone https://github.com/ShivamShrirao/Image-Captioning-Transformers"
      ],
      "id": "ujdZEDZJnpe3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Image-Captioning-Transformers'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 49 (delta 18), reused 41 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "819264fd-bf0c-4862-9dbd-c4d2504a7071"
      },
      "source": [
        "# Download Dataset and Annotations"
      ],
      "id": "819264fd-bf0c-4862-9dbd-c4d2504a7071"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKeB5Z8UYU8A"
      },
      "source": [
        "!mkdir ~/.kaggle/\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "cKeB5Z8UYU8A",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IOQUsLEHD2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9f4959-9c0a-4fa3-ed06-0ab602f21544"
      },
      "source": [
        "!kaggle datasets download -d shivamshrirao/coco-trainval2017-320x320"
      ],
      "id": "9IOQUsLEHD2z",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading coco-trainval2017-320x320.zip to /content\n",
            "100% 3.46G/3.46G [01:26<00:00, 40.9MB/s]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCLNX6TaYt4d"
      },
      "source": [
        "!unzip -q coco-trainval2017-320x320.zip"
      ],
      "id": "vCLNX6TaYt4d",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veRLNXL-xtYR"
      },
      "source": [
        "# !gdown --id 1-3vdwBlY-CdVultkrFwOhyJTGC5TFUV8"
      ],
      "id": "veRLNXL-xtYR",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbpjO-f9wWQt"
      },
      "source": [
        "# !pigz -dc coco_trainval2017_320x320.tar.gz | tar xf -"
      ],
      "id": "WbpjO-f9wWQt",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39cda7a8-5913-4ff8-a871-d56f9cc16750"
      },
      "source": [
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "DATA_DIR = \"datasets/COCO\""
      ],
      "id": "39cda7a8-5913-4ff8-a871-d56f9cc16750",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "0898b33c61064564add9813b3a7a1978",
            "056252a2d78945ed974a03cccf1bce37",
            "032a94865b6d47e097aca3c8feae040b",
            "8f54523326894828b41674e84aafdcae",
            "65e1f173865d49088a7d60353b102eb4",
            "1e0d62b01afd41109863898faf1b42ed",
            "c81d5fb60b2c4cf281657796c4a7e0e7",
            "87372d37c5404861aac5018f51c9e2e6"
          ]
        },
        "id": "e91386a8-b174-4720-af37-a6b2704eb269",
        "outputId": "a0616c49-b7a2-4d49-dd0c-801ed0225b93"
      },
      "source": [
        "download_and_extract_archive(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
        "                             download_root=DATA_DIR,\n",
        "                             remove_finished=True)"
      ],
      "id": "e91386a8-b174-4720-af37-a6b2704eb269",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://images.cocodataset.org/annotations/annotations_trainval2017.zip to datasets/COCO/annotations_trainval2017.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0898b33c61064564add9813b3a7a1978",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=252907541.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting datasets/COCO/annotations_trainval2017.zip to datasets/COCO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWKDs4aIux4R"
      },
      "source": [
        "!rm coco-trainval2017-320x320.* datasets/COCO/annotations_trainval2017.zip"
      ],
      "id": "qWKDs4aIux4R",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcFgDzJP8MO4"
      },
      "source": [
        "# Import libraries"
      ],
      "id": "zcFgDzJP8MO4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW7EJRTVnwll",
        "outputId": "6bd4a58f-5c60-41f8-e82e-1e763e686563"
      },
      "source": [
        "%cd Image-Captioning-Transformers"
      ],
      "id": "qW7EJRTVnwll",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Image-Captioning-Transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ffdef7-f21f-4450-ae9a-c43b7f79cba0"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "id": "07ffdef7-f21f-4450-ae9a-c43b7f79cba0",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98a58c51-77fb-48f7-a6bf-52e6dcd8f93a"
      },
      "source": [
        "# TODO: Try pre trained CLIP"
      ],
      "id": "98a58c51-77fb-48f7-a6bf-52e6dcd8f93a",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aae2eb10-f8d5-4f47-bccf-c29faf2cdbcd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "id": "aae2eb10-f8d5-4f47-bccf-c29faf2cdbcd",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6332628b-660d-4867-b690-e0642674c4ac"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "id": "6332628b-660d-4867-b690-e0642674c4ac",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcb4177c-640f-469c-9ac4-d09a19ee8e37"
      },
      "source": [
        "import timm         # torch image models"
      ],
      "id": "fcb4177c-640f-469c-9ac4-d09a19ee8e37",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc18b9ba-75cb-4b7a-a909-cb226522e981"
      },
      "source": [
        "plt.rcParams['figure.facecolor'] = 'white'"
      ],
      "id": "dc18b9ba-75cb-4b7a-a909-cb226522e981",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f978fc3-2bca-40cf-af8a-263371f97577"
      },
      "source": [
        "# Wandb Parameters"
      ],
      "id": "2f978fc3-2bca-40cf-af8a-263371f97577"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6f8f439-401b-4dd3-ada8-fbbfcf2beded"
      },
      "source": [
        "import wandb"
      ],
      "id": "e6f8f439-401b-4dd3-ada8-fbbfcf2beded",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b59efe92-566d-46ab-a91a-6323a855ff40"
      },
      "source": [
        "config_defaults = {\n",
        "    'BATCH_SIZE'        : 256,\n",
        "    'd_model'           : 768,\n",
        "    'dim_feedforward'   : 1536,\n",
        "    'nheads'            : 12,\n",
        "    'num_decoder_layers': 4,\n",
        "    'dp_rate'           : 0.1,\n",
        "    'encoder'           : 'seresnext50_32x4d',\n",
        "    'activation'        : 'gelu',\n",
        "    'max_lr'            : 5e-4,\n",
        "    'betas'             : (0.9, 0.98),\n",
        "    'eps'               : 1e-9,\n",
        "    'seed'              : 62134,\n",
        "    'use_amp'           : True,\n",
        "    'use_pe'            : True,\n",
        "    'log_interval'      : 10,\n",
        "}\n",
        "CONFIG = config_defaults"
      ],
      "id": "b59efe92-566d-46ab-a91a-6323a855ff40",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b5aca5c-bf25-45b8-b3e3-73652fa09a3d"
      },
      "source": [
        "# #hide\n",
        "# run = wandb.init(id='3vhov6z0', project=\"Image_Captioning_Transformer\", resume='must')\n",
        "# CONFIG = run.config"
      ],
      "id": "3b5aca5c-bf25-45b8-b3e3-73652fa09a3d",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09f936c7-fd7f-42f6-89d9-d0b4468fb48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5b4135eb-59df-4106-b230-d53bae84770d"
      },
      "source": [
        "run = wandb.init(project=\"Image_Captioning_Transformer\", entity=\"shivamshrirao\", config=config_defaults)\n",
        "CONFIG = wandb.config"
      ],
      "id": "09f936c7-fd7f-42f6-89d9-d0b4468fb48c",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshivamshrirao\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.32<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">glowing-glitter-26</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/shivamshrirao/Image_Captioning_Transformer\" target=\"_blank\">https://wandb.ai/shivamshrirao/Image_Captioning_Transformer</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/shivamshrirao/Image_Captioning_Transformer/runs/1io0u533\" target=\"_blank\">https://wandb.ai/shivamshrirao/Image_Captioning_Transformer/runs/1io0u533</a><br/>\n",
              "                Run data is saved locally in <code>/content/Image-Captioning-Transformers/wandb/run-20210627_213618-1io0u533</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mfbg5gKZ8-E"
      },
      "source": [
        "def seed_everything(seed=33):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(CONFIG['seed'])"
      ],
      "id": "_mfbg5gKZ8-E",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da9a4f51-0e2b-40fc-ac16-258f576b2794"
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "da9a4f51-0e2b-40fc-ac16-258f576b2794",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e2f199d-ba6f-4d12-a3b0-76a5f987f0c9"
      },
      "source": [
        "## Read COCO dataset"
      ],
      "id": "9e2f199d-ba6f-4d12-a3b0-76a5f987f0c9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JH4w01G0dhe"
      },
      "source": [
        "from imcap.dataset import *"
      ],
      "id": "6JH4w01G0dhe",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_20KeTj8rk0"
      },
      "source": [
        "DATA_DIR = \"../datasets/COCO/\""
      ],
      "id": "G_20KeTj8rk0",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ca3492-26a4-4310-bec7-44cec5092a62",
        "outputId": "aa78bbf9-d24e-4a9a-f46b-aac9da2f5b9a"
      },
      "source": [
        "train_data = TensorCocoCaptions(root=DATA_DIR+\"/train2017/\",\n",
        "                                annFile=DATA_DIR+\"/annotations/captions_train2017.json\")\n",
        "\n",
        "val_data = TensorCocoCaptions(root=DATA_DIR+\"/val2017/\",\n",
        "                              annFile=DATA_DIR+\"/annotations/captions_val2017.json\")"
      ],
      "id": "96ca3492-26a4-4310-bec7-44cec5092a62",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.05s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93d1c495-f3d0-4b58-bb88-286530849969"
      },
      "source": [
        "## Tokenizer and Build Vocab"
      ],
      "id": "93d1c495-f3d0-4b58-bb88-286530849969"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKNjis9B0xHg"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer"
      ],
      "id": "GKNjis9B0xHg",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4e10704-bd29-465c-8626-b0a48ee2f01b"
      },
      "source": [
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "id": "b4e10704-bd29-465c-8626-b0a48ee2f01b",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c84dd7aa-6686-4e2f-8537-919460d0a12c"
      },
      "source": [
        "def yield_tokens(cap_data):\n",
        "    for ann in cap_data.coco.anns.values():\n",
        "        yield tokenizer(ann['caption'])"
      ],
      "id": "c84dd7aa-6686-4e2f-8537-919460d0a12c",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8922a7a-5747-4a04-95fc-976a6947a5ba"
      },
      "source": [
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "en_vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=special_symbols, special_first=True)\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = en_vocab(special_symbols)\n",
        "en_vocab.set_default_index(UNK_IDX)"
      ],
      "id": "f8922a7a-5747-4a04-95fc-976a6947a5ba",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRJA2NEq52Rh",
        "outputId": "18db9805-d442-4841-840d-d4519b6c3ec9"
      },
      "source": [
        "len(en_vocab)"
      ],
      "id": "wRJA2NEq52Rh",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28940"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MFKWGiOB9K5",
        "outputId": "2322449b-fbd0-4f6c-dfb8-69016913f69e"
      },
      "source": [
        "train_data.fill_token_dict(tokenizer, en_vocab, BOS_IDX, EOS_IDX)\n",
        "val_data.fill_token_dict(tokenizer, en_vocab, BOS_IDX, EOS_IDX)"
      ],
      "id": "-MFKWGiOB9K5",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 118287/118287 [00:18<00:00, 6517.93it/s]\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 6739.17it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d493c53-ce30-443e-936e-a1dac5270477"
      },
      "source": [
        "## Pretrained Glove Embeddings (not used rn)"
      ],
      "id": "6d493c53-ce30-443e-936e-a1dac5270477"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "042950b8-a6d3-494d-997f-1edbc2e53339"
      },
      "source": [
        "# vec.get_vecs_by_tokens(tokens, lower_case_backup=True)"
      ],
      "id": "042950b8-a6d3-494d-997f-1edbc2e53339",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "893f50d5-adff-459f-a09c-10e001e48d23"
      },
      "source": [
        "# vec = torchtext.vocab.GloVe('6B', dim=300)\n",
        "# unk_vec = vec.vectors.mean(dim=0)\n",
        "# vec.unk_init = lambda x: unk_vec"
      ],
      "id": "893f50d5-adff-459f-a09c-10e001e48d23",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a7920ad-38a9-4fc7-9878-a0765ddac8fd"
      },
      "source": [
        "# Load dataset into batches"
      ],
      "id": "8a7920ad-38a9-4fc7-9878-a0765ddac8fd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aazPtQVh2Aw"
      },
      "source": [
        "from imcap.dataloader import *"
      ],
      "id": "9aazPtQVh2Aw",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puscj06UBKJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8e3766-cf3d-43ca-9e1f-1e6f294f0db5"
      },
      "source": [
        "train_iter = ExternalInputIterator(train_data, CONFIG['BATCH_SIZE'], PAD_IDX)\n",
        "pipe = ExternalSourcePipeline(batch_size=CONFIG['BATCH_SIZE'], num_threads=4, device_id=0, external_data=train_iter, input_size=input_size)\n",
        "train_loader = DALIClassificationIterator(pipe, dynamic_shape=True, auto_reset=True, last_batch_padded=True, size=len(train_iter))\n",
        "\n",
        "val_iter = ExternalInputIterator(val_data, CONFIG['BATCH_SIZE'], PAD_IDX, training=False)\n",
        "pipe = ExternalSourcePipeline(batch_size=CONFIG['BATCH_SIZE'], num_threads=4, device_id=0, external_data=val_iter, input_size=input_size, training=False)\n",
        "val_loader = DALIClassificationIterator(pipe, dynamic_shape=True, auto_reset=True, last_batch_padded=True, size=len(val_iter))"
      ],
      "id": "puscj06UBKJl",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nvidia/dali/plugin/base_iterator.py:162: Warning: Please set `reader_name` and don't set last_batch_padded and size manually whenever possible. This may lead, in some situations, to missing some samples or returning duplicated ones. Check the Sharding section of the documentation for more details.\n",
            "  _iterator_deprecation_warning()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0c04da7-ebc6-4ea8-b803-e5529df1668b"
      },
      "source": [
        "# Initialize Model"
      ],
      "id": "d0c04da7-ebc6-4ea8-b803-e5529df1668b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c58a2731-9751-47bf-a2f1-55a9c008ac33"
      },
      "source": [
        "from imcap.layers import *\n",
        "from imcap.utils import *"
      ],
      "id": "c58a2731-9751-47bf-a2f1-55a9c008ac33",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c54e00a1-b9e4-4841-befe-6b55ec0f75fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddca1cc-c1e5-4421-8a88-a4282f885ef5"
      },
      "source": [
        "model = CaptionModel(encoder = timm.create_model(CONFIG['encoder'], pretrained=True, num_classes=0, global_pool=''),\n",
        "                     vocab_size = len(en_vocab),\n",
        "                     num_decoder_layers = CONFIG['num_decoder_layers'],\n",
        "                     nheads = CONFIG['nheads'],\n",
        "                     d_model = CONFIG['d_model'],\n",
        "                     dim_feedforward = CONFIG['dim_feedforward'],\n",
        "                     dp_rate = CONFIG['dp_rate'],\n",
        "                     activation = CONFIG['activation']).to(DEVICE, non_blocking=True)"
      ],
      "id": "c54e00a1-b9e4-4841-befe-6b55ec0f75fd",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08b4b6bf-c036-429f-b5e5-1d053f3d91e9"
      },
      "source": [
        "# Learning Rate Schedule"
      ],
      "id": "08b4b6bf-c036-429f-b5e5-1d053f3d91e9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5639c04c-f3f9-4c28-8bff-cdd7bc2fc02a"
      },
      "source": [
        "steps_per_epoch = len(train_loader)"
      ],
      "id": "5639c04c-f3f9-4c28-8bff-cdd7bc2fc02a",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6336bdd7-aee5-4bd7-b382-cb18e023aa8f"
      },
      "source": [
        "# def lr_schedule(step, d_model=512, warmup_steps=2*steps_per_epoch):\n",
        "#     # return 1\n",
        "#     step = max(1,step)\n",
        "#     arg1 = step ** -0.5\n",
        "#     arg2 = step * (warmup_steps ** -1.5)\n",
        "#     return (d_model ** -0.6) * min(arg1, arg2)"
      ],
      "id": "6336bdd7-aee5-4bd7-b382-cb18e023aa8f",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceca2478-81ff-402f-b984-72c19e0391d0"
      },
      "source": [
        "# plt.plot(list(map(lr_schedule, range(steps_per_epoch*50))))\n",
        "# plt.show()"
      ],
      "id": "ceca2478-81ff-402f-b984-72c19e0391d0",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBR3qr5gd3SI"
      },
      "source": [
        "# plt.plot([scheduler.get_last_lr()[0] for _ in range(steps_per_epoch*50) if not scheduler.step()])\n",
        "# plt.show()"
      ],
      "id": "PBR3qr5gd3SI",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc37d126-9abb-4a3e-8304-cd589dee058e"
      },
      "source": [
        "# Loss Function and Optimizer"
      ],
      "id": "dc37d126-9abb-4a3e-8304-cd589dee058e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "530023ee-432b-4ed8-ba6d-1831dffaebf6"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=CONFIG['max_lr'], betas=CONFIG['betas'], eps=CONFIG['eps']\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CONFIG['max_lr'], total_steps=50*steps_per_epoch, pct_start=0.01, final_div_factor=0.31)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=CONFIG['use_amp'])"
      ],
      "id": "530023ee-432b-4ed8-ba6d-1831dffaebf6",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7734054-af8a-4823-8b50-66f2e6c86b97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c60a56a-e40b-4fe6-9920-2916e8d86870"
      },
      "source": [
        "wandb.watch(model, log=None)"
      ],
      "id": "f7734054-af8a-4823-8b50-66f2e6c86b97",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<wandb.wandb_torch.TorchGraph at 0x7f39524c1a10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTJuKcv07aBa"
      },
      "source": [
        "# Training functions"
      ],
      "id": "dTJuKcv07aBa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95a5da74-3381-46df-aeb0-3d2f6632d97a"
      },
      "source": [
        "from torch.cuda import amp"
      ],
      "id": "95a5da74-3381-46df-aeb0-3d2f6632d97a",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83b197ac-b9f7-46d9-be5b-a069e61a500b"
      },
      "source": [
        "def train_epoch(model, train_loader, optimizer, scaler, scheduler, epoch=1, use_amp=True, log_interval=10):\n",
        "    model.train()\n",
        "    model.encoder.eval()\n",
        "    losses = 0\n",
        "    with tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\") as pbar:\n",
        "        for idx, batch in pbar:\n",
        "            img, tgt = batch[0]['data'], batch[0]['label'].transpose(0,1)\n",
        "            # img = img.to(DEVICE, non_blocking=True)\n",
        "            # tgt = tgt.to(DEVICE, non_blocking=True)\n",
        "            \n",
        "            tgt_inp = tgt[:-1,:]      # give input until before the last word.\n",
        "            tgt_out = tgt[1:, :]      # predict the last word based on input and already predicted sentence. (auto-regressive)\n",
        "\n",
        "            tgt_mask, tgt_pad_mask = subsequent_mask(tgt_inp.size(0), DEVICE), padding_mask(tgt_inp, PAD_IDX)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with amp.autocast(enabled=use_amp):\n",
        "                logits = model(img, tgt_inp, tgt_mask, tgt_pad_mask)\n",
        "                loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            losses+= loss.detach_()\n",
        "            # del loss, logits, batch, img\n",
        "\n",
        "            if not idx%log_interval:\n",
        "                curr_lr = optimizer.param_groups[0]['lr']\n",
        "                losses = float(losses)\n",
        "                info = {'loss': losses/(idx+1), 'lr': curr_lr}\n",
        "                wandb.log(info)\n",
        "                pbar.set_postfix(info)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    return float(losses)/len(train_loader)"
      ],
      "id": "83b197ac-b9f7-46d9-be5b-a069e61a500b",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06c126d-1c88-4119-a1c9-2a3cd78d4e9b"
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader, use_amp=True):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    with tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Evaluating\") as pbar:\n",
        "        for idx, batch in pbar:\n",
        "            img, tgt = batch[0]['data'], batch[0]['label'].transpose(0,1)\n",
        "            # img = img.to(DEVICE, non_blocking=True)\n",
        "            # tgt = tgt.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            tgt_inp = tgt[:-1,:]      # give input until before the last word.\n",
        "            tgt_out = tgt[1:, :]      # predict the last word based on input and already predicted sentence. (auto-regressive)\n",
        "\n",
        "            tgt_mask, tgt_pad_mask = subsequent_mask(tgt_inp.size(0), DEVICE), padding_mask(tgt_inp, PAD_IDX)\n",
        "            \n",
        "            with amp.autocast(enabled=use_amp):\n",
        "                logits = model(img, tgt_inp, tgt_mask, tgt_pad_mask)\n",
        "                loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "\n",
        "            losses+= float(loss.detach_())\n",
        "            pbar.set_postfix({'val_loss': losses/(idx+1)})\n",
        "    return float(losses)/len(val_loader)"
      ],
      "id": "c06c126d-1c88-4119-a1c9-2a3cd78d4e9b",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f954d0ba-cc4e-4b9d-bec3-202183c517ca"
      },
      "source": [
        "# Functions to Make Predictions"
      ],
      "id": "f954d0ba-cc4e-4b9d-bec3-202183c517ca"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34341b74-edb8-4e7c-8f05-43b3c0f0db2a"
      },
      "source": [
        "@torch.no_grad()\n",
        "def greedy_decode(model, img, max_len=100, start_symbol=BOS_IDX):\n",
        "    model.eval()\n",
        "    img = img.to(DEVICE, non_blocking=True)\n",
        "    enc_output = model.encode_image(img)\n",
        "    tgt = torch.ones(1, 1).fill_(start_symbol).long().to(DEVICE, non_blocking=True)\n",
        "    for i in range(max_len):\n",
        "        tgt_mask = subsequent_mask(tgt.size(0), DEVICE)\n",
        "        out = model.decode_text(tgt, enc_output, tgt_mask)\n",
        "        out = out.transpose(0,1)\n",
        "        prob = model.generator(out[:,-1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "        tgt = torch.cat([tgt, torch.ones(1, 1).fill_(next_word).long().to(DEVICE)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return tgt.detach()\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_caption(model, img, tgt_vocab):\n",
        "    tgt = greedy_decode(model, img, max_len=100, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(tgt_vocab.lookup_tokens(tgt.tolist())).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "id": "34341b74-edb8-4e7c-8f05-43b3c0f0db2a",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGEcLDCH7g7E"
      },
      "source": [
        "# Begin Training"
      ],
      "id": "GGEcLDCH7g7E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53012613-4bdd-4e1f-abe1-e9c4ee9fa3d3"
      },
      "source": [
        "init_epoch = 1\n",
        "NUM_EPOCHS = 50"
      ],
      "id": "53012613-4bdd-4e1f-abe1-e9c4ee9fa3d3",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb9476c9-376f-49f5-a3bd-881210d50d0d"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "id": "cb9476c9-376f-49f5-a3bd-881210d50d0d",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4391b037-7a0a-4179-82e9-d7e9e1ff6c11"
      },
      "source": [
        "import glob\n",
        "val_paths = glob.glob(\"../datasets/COCO/val2017/*\")"
      ],
      "id": "4391b037-7a0a-4179-82e9-d7e9e1ff6c11",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwpPA5d7o7bK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54664fb4-8ea9-4181-f3c3-cbbc34a5dcbb"
      },
      "source": [
        "#collapse-output\n",
        "for epoch in range(init_epoch, NUM_EPOCHS+1):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, scaler, scheduler,\n",
        "                             epoch, CONFIG['use_amp'], CONFIG['log_interval'])\n",
        "    # with torch.no_grad():\n",
        "    val_loss = evaluate(model, val_loader, CONFIG['use_amp'])\n",
        "\n",
        "    img = Image.open(random.choice(val_paths))\n",
        "    caps = generate_caption(model, preproc['val'](img)[None,:], en_vocab)\n",
        "    wandb.log({\"val_loss\": val_loss, \"epoch\": epoch, \"examples\": wandb.Image(img, caption=caps)})\n",
        "    print(f\"\\nEpoch: {epoch}/{NUM_EPOCHS}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\\n\")\n",
        "    gc.collect()\n",
        "    # if not epoch%10:\n",
        "    #     save_model(model, optimizer, epoch)"
      ],
      "id": "qwpPA5d7o7bK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 463/463 [07:59<00:00,  1.04s/it, loss=3.92, lr=0.0005]\n",
            "Evaluating: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s, val_loss=2.54]\n",
            "Epoch 2:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/50, Train loss: 3.917, Val loss: 2.538\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 463/463 [07:59<00:00,  1.04s/it, loss=2.68, lr=0.000499]\n",
            "Evaluating: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s, val_loss=2.36]\n",
            "Epoch 3:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2/50, Train loss: 2.678, Val loss: 2.357\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 463/463 [07:54<00:00,  1.03s/it, loss=2.54, lr=0.000497]\n",
            "Evaluating: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s, val_loss=2.29]\n",
            "Epoch 4:   0%|          | 0/463 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3/50, Train loss: 2.541, Val loss: 2.287\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4:  18%|█▊        | 82/463 [01:26<06:45,  1.06s/it, loss=2.48, lr=0.000497]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0694d7b8-8858-40c4-9aa5-06af829377f9"
      },
      "source": [
        "init_epoch = epoch\n",
        "init_epoch"
      ],
      "id": "0694d7b8-8858-40c4-9aa5-06af829377f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0a69310-dc2b-4bf5-a8e4-35865105342c"
      },
      "source": [
        "# def save_model(model, optimizer, epoch):\n",
        "#     torch.save({\n",
        "#                 'model_state_dict': model.state_dict(),\n",
        "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                 'epoch': epoch,\n",
        "#                 }, '/content/model.pth')"
      ],
      "id": "e0a69310-dc2b-4bf5-a8e4-35865105342c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFfXPsVtLai7"
      },
      "source": [
        "# Make Predictions"
      ],
      "id": "zFfXPsVtLai7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI44iCZFKLKH"
      },
      "source": [
        "img = Image.open(random.choice(val_paths))\n",
        "caps = generate_caption(model, preproc['val'](img)[None,:], en_vocab)\n",
        "# wandb.log({\"examples\": wandb.Image(img, caption=caps)})\n",
        "print(caps)\n",
        "img"
      ],
      "id": "AI44iCZFKLKH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17db2ed5-573d-4926-8567-3637b42338ae"
      },
      "source": [
        "run.finish()"
      ],
      "id": "17db2ed5-573d-4926-8567-3637b42338ae",
      "execution_count": null,
      "outputs": []
    }
  ]
}